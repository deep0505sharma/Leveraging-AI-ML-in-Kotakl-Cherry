{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.keras import Sequential\n",
    "from tensorflow.python.keras.layers import Dropout, Dense, LSTM\n",
    "\n",
    "\n",
    "class LongShortTermMemory:\n",
    "    def __init__(self, project_folder):\n",
    "        self.project_folder = project_folder\n",
    "\n",
    "    def get_defined_metrics(self):\n",
    "        defined_metrics = [\n",
    "            tf.keras.metrics.MeanSquaredError(name='MSE')\n",
    "        ]\n",
    "        return defined_metrics\n",
    "\n",
    "    def get_callback(self):\n",
    "        callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3, mode='min', verbose=1)\n",
    "        return callback\n",
    "\n",
    "    def create_model(self, x_train):\n",
    "        model = Sequential()\n",
    "        # 1st layer with Dropout regularisation\n",
    "        # * units = add 100 neurons is the dimensionality of the output space\n",
    "        # * return_sequences = True to stack LSTM layers so the next LSTM layer has a three-dimensional sequence input\n",
    "        # * input_shape => Shape of the training dataset\n",
    "        model.add(LSTM(units=100, return_sequences=True, input_shape=(x_train.shape[1], 1)))\n",
    "        # 20% of the layers will be dropped\n",
    "        model.add(Dropout(0.2))\n",
    "        # 2nd LSTM layer\n",
    "        # * units = add 50 neurons is the dimensionality of the output space\n",
    "        # * return_sequences = True to stack LSTM layers so the next LSTM layer has a three-dimensional sequence input\n",
    "        model.add(LSTM(units=50, return_sequences=True))\n",
    "        # 20% of the layers will be dropped\n",
    "        model.add(Dropout(0.2))\n",
    "        # 3rd LSTM layer\n",
    "        # * units = add 50 neurons is the dimensionality of the output space\n",
    "        # * return_sequences = True to stack LSTM layers so the next LSTM layer has a three-dimensional sequence input\n",
    "        model.add(LSTM(units=50, return_sequences=True))\n",
    "        # 50% of the layers will be dropped\n",
    "        model.add(Dropout(0.5))\n",
    "        # 4th LSTM layer\n",
    "        # * units = add 50 neurons is the dimensionality of the output space\n",
    "        model.add(LSTM(units=50))\n",
    "        # 50% of the layers will be dropped\n",
    "        model.add(Dropout(0.5))\n",
    "        # Dense layer that specifies an output of one unit\n",
    "        model.add(Dense(units=1))\n",
    "        model.summary()\n",
    "        #tf.keras.utils.plot_model(model, to_file=os.path.join(self.project_folder, 'model_lstm.png'), show_shapes=True,\n",
    "        #                          show_layer_names=True)\n",
    "        return model"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
